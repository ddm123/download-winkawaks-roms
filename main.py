import urllib.request
import urllib.error
import urllib.parse
import html.parser
import os
import time
import re
import http.client

class RomDownloader:
    def __init__(self, base_url, download_dir="downloads"):
        self.base_url = base_url
        self.download_dir = download_dir
        
        # è®¾ç½®è¯·æ±‚å¤´
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
    
    def _make_request(self, url, method='GET', headers=None, data=None):
        """ä½¿ç”¨urllibå‘é€HTTPè¯·æ±‚"""
        if headers is None:
            headers = {}
        
        # åˆå¹¶headers
        request_headers = {**self.headers, **headers}
        
        # åˆ›å»ºè¯·æ±‚
        if data:
            data = urllib.parse.urlencode(data).encode()
        
        req = urllib.request.Request(url, data=data, headers=request_headers, method=method)
        
        try:
            response = urllib.request.urlopen(req, timeout=30)
            return response
        except urllib.error.HTTPError as e:
            return e  # è¿”å›HTTPé”™è¯¯å¯¹è±¡
        except Exception as e:
            raise e
    
    def download_file_with_retry(self, url, filename, system_dir, max_retries=3):
        """ä¸‹è½½æ–‡ä»¶ï¼Œæ”¯æŒé‡è¯•å’Œæ–­ç‚¹ç»­ä¼ """
        # åˆ›å»ºç³»ç»Ÿç›®å½•
        os.makedirs(system_dir, exist_ok=True)
        filepath = os.path.join(system_dir, filename)
        
        for attempt in range(max_retries):
            try:
                success = self._download_file_resume(url, filepath, attempt + 1)
                if success:
                    return True
                else:
                    if attempt < max_retries - 1:
                        print(f"ğŸ”„ é‡è¯•ä¸‹è½½ ({attempt + 2}/{max_retries})")
                        time.sleep(2)
                    
            except Exception as e:
                print(f"âŒ ä¸‹è½½å‡ºé”™: {e}")
                if attempt < max_retries - 1:
                    print(f"ğŸ”„ å‡†å¤‡é‡è¯• ({attempt + 2}/{max_retries})")
                    time.sleep(2)
        
        print(f"ğŸ’¥ ä¸‹è½½å¤±è´¥ï¼Œå·²é‡è¯•{max_retries}æ¬¡: {os.path.basename(system_dir)}/{filename}")
        
        # å¦‚æœæœ€ç»ˆå¤±è´¥ï¼Œåˆ é™¤å¯èƒ½æŸåçš„ç©ºç™½æ–‡ä»¶
        if os.path.exists(filepath) and os.path.getsize(filepath) == 0:
            try:
                os.remove(filepath)
            except:
                pass
            
        return False
    
    def _download_file_resume(self, url, filepath, attempt_num):
        """æ”¯æŒæ–­ç‚¹ç»­ä¼ çš„æ–‡ä»¶ä¸‹è½½"""
        try:
            # è·å–å·²ä¸‹è½½çš„æ–‡ä»¶å¤§å°
            existing_size = 0
            file_exists = os.path.exists(filepath)
            
            if file_exists:
                existing_size = os.path.getsize(filepath)
                if existing_size > 0:
                    print(f"ğŸ“ å‘ç°å·²ä¸‹è½½æ–‡ä»¶: {self._format_size(existing_size)}")
            
            # è·å–æœåŠ¡å™¨æ–‡ä»¶ä¿¡æ¯
            server_total_size = 0
            try:
                # ä½¿ç”¨HEADè¯·æ±‚è·å–æ–‡ä»¶ä¿¡æ¯
                head_response = self._make_request(url, method='HEAD')
                if hasattr(head_response, 'status') and head_response.status == 200:
                    server_total_size = int(head_response.headers.get('Content-Length', 0))
                    print(f"ğŸŒ æœåŠ¡å™¨æ–‡ä»¶å¤§å°: {self._format_size(server_total_size)}")
            except Exception as e:
                print(f"âš ï¸ æ— æ³•è·å–æœåŠ¡å™¨æ–‡ä»¶ä¿¡æ¯: {e}")
                raise
            
            headers = {}
            resume_download = False
            
            if server_total_size > 0:
                if existing_size == server_total_size:
                    # âœ… æƒ…å†µ1: æ–‡ä»¶å·²å®Œæ•´ä¸‹è½½
                    print(f"â­ï¸ æ–‡ä»¶å·²å®Œæ•´ï¼Œè·³è¿‡ä¸‹è½½")
                    return True
                    
                elif existing_size > 0 and existing_size < server_total_size:
                    # âœ… æƒ…å†µ2: æ–‡ä»¶å·²éƒ¨åˆ†ä¸‹è½½ï¼Œéœ€è¦ç»­ä¼ 
                    headers['Range'] = f'bytes={existing_size}-'
                    resume_download = True
                    remaining_size = server_total_size - existing_size
                    print(f"ğŸ”„ ç»§ç»­ä¸‹è½½: å‰©ä½™ {self._format_size(remaining_size)}")
                    
                elif existing_size > server_total_size:
                    # âœ… æƒ…å†µ3: æœåŠ¡å™¨æ–‡ä»¶æ›´æ–°ï¼Œåˆ é™¤é‡ä¸‹
                    print(f"ğŸ”„ æœåŠ¡å™¨æ–‡ä»¶å·²æ›´æ–°ï¼Œåˆ é™¤æ—§æ–‡ä»¶é‡æ–°ä¸‹è½½")
                    os.remove(filepath)
                    existing_size = 0
                    file_exists = False
                    
            else:
                # æ— æ³•è·å–æœåŠ¡å™¨å¤§å°çš„æƒ…å†µ
                if file_exists and existing_size > 1024:
                    print(f"âš ï¸ æ— æ³•éªŒè¯æœåŠ¡å™¨å¤§å°ï¼Œä½†æ–‡ä»¶å·²å­˜åœ¨: {self._format_size(existing_size)}")
                    return True
                elif file_exists:
                    print(f"ğŸ”„ æ–‡ä»¶å¤ªå°å¯èƒ½ä¸å®Œæ•´ï¼Œé‡æ–°ä¸‹è½½")
                    os.remove(filepath)
                    existing_size = 0
            
            print(f"ğŸ“¥ ä¸‹è½½({attempt_num}): {os.path.basename(os.path.dirname(filepath))}/{os.path.basename(filepath)}")
            
            # æ‰§è¡Œä¸‹è½½
            response = self._make_request(url, headers=headers)
            
            # æ£€æŸ¥å“åº”çŠ¶æ€
            if hasattr(response, 'status'):
                status_code = response.status
            else:
                status_code = 200  # urlopenæˆåŠŸé»˜è®¤æ˜¯200
            
            # å¤„ç†æœåŠ¡å™¨ä¸æ”¯æŒæ–­ç‚¹ç»­ä¼ çš„æƒ…å†µ
            if resume_download and status_code != 206:
                print("âš ï¸ æœåŠ¡å™¨ä¸æ”¯æŒæ–­ç‚¹ç»­ä¼ ï¼Œé‡æ–°ä¸‹è½½")
                try:
                    os.remove(filepath)  # åˆ é™¤å¯èƒ½æŸåçš„æ–‡ä»¶
                except:
                    pass
                existing_size = 0
                resume_download = False
                response = self._make_request(url)
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯é”™è¯¯å“åº”
            if hasattr(response, 'status') and response.status >= 400:
                raise urllib.error.HTTPError(url, response.status, response.reason, response.headers, response)
            
            # ç¡®å®šæ€»æ–‡ä»¶å¤§å°
            total_size = 0
            if resume_download and 'Content-Range' in response.headers:
                content_range = response.headers['Content-Range']
                total_size = int(content_range.split('/')[-1])
            else:
                total_size = int(response.headers.get('Content-Length', 0))
                if resume_download:
                    total_size += existing_size
            
            # ä¸‹è½½æ–‡ä»¶
            mode = 'ab' if resume_download else 'wb'
            downloaded = existing_size if resume_download else 0
            
            with open(filepath, mode) as file:
                start_time = time.time()
                last_update = start_time
                
                while True:
                    chunk = response.read(8192)
                    if not chunk:
                        break
                    
                    file.write(chunk)
                    downloaded += len(chunk)
                    
                    # æ˜¾ç¤ºè¿›åº¦ï¼ˆé™åˆ¶æ›´æ–°é¢‘ç‡ï¼‰
                    current_time = time.time()
                    if current_time - last_update > 0.5 or downloaded == total_size:
                        if total_size > 0:
                            percent = (downloaded / total_size) * 100
                            elapsed = current_time - start_time
                            speed = downloaded / elapsed / 1024 if elapsed > 0 else 0
                            print(f"\rğŸ“Š è¿›åº¦: {percent:.2f}% ({self._format_size(downloaded)}/{self._format_size(total_size)}) - {speed:.2f}KB/s", end='', flush=True)
                        last_update = current_time
            
            # æœ€ç»ˆå®Œæ•´æ€§éªŒè¯
            final_size = os.path.getsize(filepath)
            if total_size > 0:
                if final_size == total_size:
                    print(f"\nâœ… ä¸‹è½½å®Œæˆï¼Œæ–‡ä»¶å¤§å°éªŒè¯é€šè¿‡")
                    return True
                else:
                    print(f"\nâŒ æ–‡ä»¶éªŒè¯å¤±è´¥: {self._format_size(final_size)} != {self._format_size(total_size)}")
                    # åˆ é™¤ä¸å®Œæ•´æ–‡ä»¶ï¼Œè®©ä¸‹æ¬¡é‡è¯•å¯ä»¥é‡æ–°å¼€å§‹
                    try:
                        os.remove(filepath)
                    except:
                        pass
                    return False
            else:
                # æ— æ³•éªŒè¯ï¼Œä½†æ–‡ä»¶ä¸ä¸ºç©ºå°±è®¤ä¸ºæˆåŠŸ
                if final_size > 0:
                    print(f"\nâœ… ä¸‹è½½å®Œæˆ: {self._format_size(final_size)}")
                    return True
                else:
                    print(f"\nâŒ ä¸‹è½½å¤±è´¥: æ–‡ä»¶ä¸ºç©º")
                    return False
            
        except urllib.error.URLError as e:
            print(f"\nâŒ ç½‘ç»œé”™è¯¯: {e}")
            return False
        except IOError as e:
            print(f"\nâŒ æ–‡ä»¶å†™å…¥é”™è¯¯: {e}")
            return False
        except Exception as e:
            print(f"\nâŒ ä¸‹è½½é”™è¯¯: {e}")
            return False
    
    def _format_size(self, size_bytes):
        """æ ¼å¼åŒ–æ–‡ä»¶å¤§å°æ˜¾ç¤º"""
        if size_bytes == 0:
            return "0B"
        
        size_names = ["B", "KB", "MB", "GB"]
        i = 0
        while size_bytes >= 1024 and i < len(size_names) - 1:
            size_bytes /= 1024.0
            i += 1
        
        return f"{size_bytes:.1f}{size_names[i]}"
    
    def get_system_from_url(self, rom_page_url):
        """ä»URLä¸­æå–ç³»ç»Ÿåç§°"""
        match = re.search(r'/roms/([^/]+)/[^/]+\.htm', rom_page_url)
        if match:
            return match.group(1)
        return "unknown"
    
    def get_download_links(self, rom_page_url, attempt_num=1):
        """è·å–ROMé¡µé¢çš„ä¸‹è½½é“¾æ¥"""
        try:
            response = self._make_request(rom_page_url)
            if hasattr(response, 'status') and response.status >= 400:
                raise urllib.error.HTTPError(rom_page_url, response.status, response.reason, response.headers, response)
            
            html_content = response.read().decode('utf-8')
            
            # ä½¿ç”¨è‡ªå®šä¹‰è§£æå™¨è§£æä¸‹è½½é“¾æ¥
            parser = LinkParser()
            parser.feed(html_content)
            download_links = []
            
            for link_info in parser.get_download_links():
                href = link_info['href']
                filename = link_info['text'] or os.path.basename(href) or f"unknown_{len(download_links)}.zip"
                
                # å¤„ç†URL
                if href.startswith('//'):
                    full_url = 'https:' + href
                elif href.startswith('/'):
                    full_url = urllib.parse.urljoin(self.base_url, href)
                elif not href.startswith(('http://', 'https://')):
                    full_url = urllib.parse.urljoin(rom_page_url, href)
                else:
                    full_url = href
                
                # æ¸…ç†æ–‡ä»¶å
                filename = "".join(c for c in filename if c.isalnum() or c in (' ', '-', '_', '.')).rstrip()
                if not filename.endswith(('.zip', '.rar', '.7z')):
                    filename += '.zip'
                
                download_links.append((full_url, filename))
            
            return download_links
            
        except Exception as e:
            if attempt_num < 3:
                print(f"ğŸ”„ è·å–ä¸‹è½½é“¾æ¥å¤±è´¥ï¼Œé‡è¯• ({attempt_num}/3): {e}")
                time.sleep(2)
                return self.get_download_links(rom_page_url, attempt_num + 1)

            print(f"âŒ è·å–ä¸‹è½½é“¾æ¥å¤±è´¥: {e}")
            return []
    
    def download_all_roms(self, max_roms=None):
        """ä¸‹è½½æ‰€æœ‰ROMæ–‡ä»¶ï¼ŒæŒ‰ç³»ç»Ÿåˆ†ç±»ä¿å­˜"""
        try:
            print("ğŸš€ å¼€å§‹ROMä¸‹è½½ä»»åŠ¡")
            print(f"ğŸ“ ä¸‹è½½ç›®å½•: {os.path.abspath(self.download_dir)}")
            
            # è·å–ä¸»é¡µé¢
            response = self._make_request(self.base_url)
            if hasattr(response, 'status') and response.status >= 400:
                raise urllib.error.HTTPError(self.base_url, response.status, response.reason, response.headers, response)
            
            html_content = response.read().decode('utf-8')
            
            # ä½¿ç”¨è‡ªå®šä¹‰è§£æå™¨è§£æROMåˆ—è¡¨
            parser = RomListParser()
            parser.feed(html_content)
            rom_entries = parser.get_rom_entries()
            
            print(f"ğŸ® æ‰¾åˆ° {len(rom_entries)} ä¸ªROM")
            print("âš¡ æ”¯æŒæ–­ç‚¹ç»­ä¼ ï¼Œå¤±è´¥è‡ªåŠ¨é‡è¯•3æ¬¡")
            
            downloaded_count = 0
            error_count = 0
            
            for i, rom_entry in enumerate(rom_entries):
                if max_roms and downloaded_count >= max_roms:
                    print(f"â¹ï¸ è¾¾åˆ°æ•°é‡é™åˆ¶: {max_roms}")
                    break
                
                try:
                    game_title = rom_entry['title']
                    rom_page_path = rom_entry['href']
                    
                    # æ„å»ºå®Œæ•´URL
                    rom_page_url = urllib.parse.urljoin(self.base_url, rom_page_path)
                    
                    # æ„å»ºä¸‹è½½é¡µé¢URL
                    if rom_page_path.endswith('.htm'):
                        download_page_path = rom_page_path[:-4] + '-download.htm'
                    else:
                        download_page_path = rom_page_path + '-download.htm'
                    
                    download_page_url = urllib.parse.urljoin(self.base_url, download_page_path)
                    
                    # è·å–ç³»ç»Ÿåç§°
                    system_name = self.get_system_from_url(rom_page_url)
                    system_dir = os.path.join(self.download_dir, system_name)
                    
                    print(f"\nğŸ¯ [{i+1}/{len(rom_entries)}] {game_title}")
                    print(f"ğŸ“‚ ç³»ç»Ÿ: {system_name}")
                    
                    # è·å–ä¸‹è½½é“¾æ¥
                    download_links = self.get_download_links(download_page_url)
                    
                    if not download_links:
                        print("âš ï¸ æ— ä¸‹è½½é“¾æ¥")
                        error_count += 1
                        continue
                    
                    print(f"ğŸ“ æ‰¾åˆ° {len(download_links)} ä¸ªæ–‡ä»¶")
                    
                    # ä¸‹è½½æ¯ä¸ªæ–‡ä»¶
                    for file_url, filename in download_links:
                        if self.download_file_with_retry(file_url, filename, system_dir):
                            downloaded_count += 1
                        else:
                            error_count += 1
                        
                        time.sleep(1)  # è¯·æ±‚é—´éš”
                
                except Exception as e:
                    print(f"âŒ å¤„ç†å¤±è´¥: {e}")
                    error_count += 1
                    continue
            
            print(f"\nğŸ‰ ä»»åŠ¡å®Œæˆ! æˆåŠŸ: {downloaded_count}, å¤±è´¥: {error_count}")
            
        except Exception as e:
            print(f"ğŸ’¥ ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")

class LinkParser(html.parser.HTMLParser):
    """è‡ªå®šä¹‰HTMLè§£æå™¨ï¼Œç”¨äºæå–ä¸‹è½½é“¾æ¥"""
    def __init__(self):
        super().__init__()
        self.download_links = []
        self.current_tag = None
        self.current_attrs = {}
    
    def handle_starttag(self, tag, attrs):
        self.current_tag = tag
        self.current_attrs = dict(attrs)
        
        if tag == 'a' and self.current_attrs.get('target') == '_blank':
            href = self.current_attrs.get('href')
            if href:
                self.download_links.append({
                    'href': href,
                    'text': ''
                })
    
    def handle_data(self, data):
        if (self.current_tag == 'a' and 
            self.current_attrs.get('target') == '_blank' and
            self.download_links):
            # ä¸ºæœ€åä¸€ä¸ªé“¾æ¥è®¾ç½®æ–‡æœ¬
            self.download_links[-1]['text'] = data.strip()
    
    def get_download_links(self):
        """è·å–è§£æåˆ°çš„ä¸‹è½½é“¾æ¥"""
        return self.download_links

class RomListParser(html.parser.HTMLParser):
    """è§£æROMåˆ—è¡¨é¡µé¢"""
    def __init__(self):
        super().__init__()
        self.rom_entries = []
        self.current_div_class = None
        self.current_div_attrs = {}
        self.current_a_href = None
        self.current_a_text = None
        self.in_target_div = False
        self.in_target_a = False
    
    def handle_starttag(self, tag, attrs):
        attrs_dict = dict(attrs)
        
        if tag == 'div':
            class_name = attrs_dict.get('class', '')
            if class_name and 'rom-system-index-entry-full' in class_name:
                self.in_target_div = True
                self.current_div_attrs = attrs_dict
                self.current_div_class = class_name
        
        elif tag == 'a' and self.in_target_div:
            self.in_target_a = True
            self.current_a_href = attrs_dict.get('href')
            self.current_a_text = ''
    
    def handle_data(self, data):
        if self.in_target_a:
            self.current_a_text += data
    
    def handle_endtag(self, tag):
        if tag == 'div' and self.in_target_div:
            if self.current_a_href:
                self.rom_entries.append({
                    'title': self.current_div_attrs.get('title', 'æœªçŸ¥æ¸¸æˆ'),
                    'href': self.current_a_href,
                    'text': self.current_a_text.strip()
                })
            self.in_target_div = False
            self.current_div_attrs = {}
            self.current_a_href = None
            self.current_a_text = None
        
        elif tag == 'a' and self.in_target_a:
            self.in_target_a = False
    
    def get_rom_entries(self):
        """è·å–è§£æåˆ°çš„ROMæ¡ç›®"""
        return self.rom_entries

def main():
    # é…ç½®å‚æ•°
    BASE_URL = 'https://www.winkawaks.org/roms/full-rom-list.htm'
    DOWNLOAD_DIR = "rom_downloads"
    MAX_ROMS = None  # æµ‹è¯•ç”¨ï¼Œè®¾ä¸ºNoneä¸‹è½½å…¨éƒ¨
    
    downloader = RomDownloader(BASE_URL, DOWNLOAD_DIR)
    downloader.download_all_roms(max_roms=MAX_ROMS)

if __name__ == "__main__":
    main()
